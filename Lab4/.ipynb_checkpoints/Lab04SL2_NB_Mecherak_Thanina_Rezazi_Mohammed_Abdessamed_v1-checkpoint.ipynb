{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CS SIL2-SIQ2 Lab04. Naïve Bayes\n",
    "\n",
    "<p style='text-align: right;font-style: italic;'>Designed by: Dr. Abdelkrime Aries</p>\n",
    "\n",
    "In this lab, we will learn about two generative models:\n",
    "- Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Team:**\n",
    "- **Member 01**: Mecherak Thanina\n",
    "- **Member 02**: Rezazi Mohammed Abdessamed\n",
    "- **Group**: SIL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, timeit\n",
    "from typing          import Tuple, List, Type\n",
    "from collections.abc import Callable\n",
    "\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.3', '2.0.3', '3.7.2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "from sklearn.naive_bayes   import CategoricalNB\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics       import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection         import train_test_split\n",
    "from sklearn.naive_bayes             import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model            import LogisticRegression\n",
    "from sklearn.tree                    import DecisionTreeClassifier\n",
    "from sklearn.metrics                 import precision_score, recall_score\n",
    "import timeit\n",
    "\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Algorithms implementation\n",
    "\n",
    "In this section, we will try to implement multinomial Naive Bayes.\n",
    "\n",
    "\n",
    "**>> Try to use \"numpy\" which will save a lot of time and effort**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset play \n",
    "\n",
    "# outlook & temperature & humidity & windy\n",
    "Xplay = np.array([\n",
    "    ['sunny'   , 'hot' , 'high'  , 'no'],\n",
    "    ['sunny'   , 'hot' , 'high'  , 'yes'],\n",
    "    ['overcast', 'hot' , 'high'  , 'no'],\n",
    "    ['rainy'   , 'mild', 'high'  , 'no'],\n",
    "    ['rainy'   , 'cool', 'normal', 'no'],\n",
    "    ['rainy'   , 'cool', 'normal', 'yes'],\n",
    "    ['overcast', 'cool', 'normal', 'yes'],\n",
    "    ['sunny'   , 'mild', 'high'  , 'no'],\n",
    "    ['sunny'   , 'cool', 'normal', 'no'],\n",
    "    ['rainy'   , 'mild', 'normal', 'no'],\n",
    "    ['sunny'   , 'mild', 'normal', 'yes'],\n",
    "    ['overcast', 'mild', 'high'  , 'yes'],\n",
    "    ['overcast', 'hot' , 'normal', 'no'],\n",
    "    ['rainy'   , 'mild', 'high'  , 'yes']\n",
    "])\n",
    "\n",
    "Yplay = np.array([\n",
    "    'no', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'no', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'yes', \n",
    "    'no'\n",
    "])\n",
    "\n",
    "len(Xplay), len(Yplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.1. Prior probability\n",
    "\n",
    "Given an output list $Y$, the probability of each class $c_k$ is estimated as:\n",
    "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
    "\n",
    "Our function must return two lists:\n",
    "- One containing the names of unique classes.\n",
    "- Another containing probabilities of unique classes of the first list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['no', 'yes'], dtype='<U3'), [-1.0296194171811581, -0.4418327522790392])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_prior(Y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: \n",
    "    cls  = np.unique(Y) # vocabulary\n",
    "    prior = []\n",
    "    for c in cls:\n",
    "        prior.append(np.log(np.count_nonzero(Y==c)/ len(Y)))\n",
    "    return cls, prior\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array(['no', 'yes'], dtype='<U3'), array([-1.02961942, -0.44183275]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "fit_prior(Yplay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Multinomial Likelihood probability\n",
    "\n",
    "Given:\n",
    "- $A$: a categorical feature\n",
    "- $V$: unique values of this feature (feature's categories)\n",
    "- $Y$: the ouput\n",
    "- $C$: the classes\n",
    "- $\\alpha$: smoothing factor\n",
    "\n",
    "calculate the log likelihood:\n",
    "$$ \\log p(A=v|Y=c_k) = \\log(|\\{ y \\in Y / y = c_k \\text{ and } A = v\\}| + \\alpha) - \\log(|\\{y = c_k\\}| + \\alpha * |V|)$$\n",
    "\n",
    "The function must:\n",
    "- add a token \"\\<UNK\\>\" to $V$\n",
    "- return feature's categories (vocabulary) $V$\n",
    "- return a matrix where rows are $V$ and colums are $C$ containing the log probability $p(V|C)$\n",
    "- when alpha=0 and v=UNK, we will have an error, in this case we will consider log probability as $-\\infty$\n",
    "- when the probability is 0, we will consider log probability as $-\\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
       "  array([[       -inf, -0.81093022],\n",
       "         [-0.91629073, -1.09861229],\n",
       "         [-0.51082562, -1.5040774 ],\n",
       "         [       -inf,        -inf]])),\n",
       " (array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
       "  array([[-2.19722458, -0.95551145],\n",
       "         [-1.09861229, -1.178655  ],\n",
       "         [-0.81093022, -1.46633707],\n",
       "         [-2.19722458, -2.56494936]])))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Multinomial Likelihood probability\n",
    "def fit_likelihood(A: np.ndarray, Y: np.ndarray, C: np.ndarray, alpha: float = 0.) -> Tuple[np.ndarray, np.ndarray]: \n",
    "    V = np.append(np.unique(A), '<UNK>')  \n",
    "    likelihood = np.zeros((len(V), len(C))) \n",
    "\n",
    "    for i, v in enumerate(V):  \n",
    "        for j, c in enumerate(C):  \n",
    "            count = np.sum((A == v) & (Y == c))  \n",
    "            total = np.sum(Y == c)  \n",
    "            if count == 0 and alpha == 0:\n",
    "                likelihood[i, j] = -np.inf\n",
    "            else:\n",
    "                likelihood[i, j] = np.log((count + alpha) / (total + alpha * len(V)))\n",
    "\n",
    "    return V, likelihood\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "#  ((array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'), \n",
    "#  array(     [[ -inf, -0.81093022],\n",
    "#             [-0.91629073, -1.09861229], \n",
    "#             [-0.51082562, -1.5040774 ], \n",
    "#             [ -inf, -inf]])), \n",
    "# (array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'), \n",
    "#  \n",
    "   #array([[-2.19722458, -0.95551145], \n",
    "#              [-1.09861229, -1.178655 ], \n",
    "#              [-0.81093022, -1.46633707], \n",
    "#              [-2.19722458, -2.56494936]])))\n",
    "#---------------------------------------------------------------------\n",
    "C_t = np.array(['no', 'yes'])\n",
    "fit_likelihood(Xplay[:, 0], Yplay, C_t), fit_likelihood(Xplay[:, 0], Yplay, C_t, alpha=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.3. Model training\n",
    "\n",
    "**Nothing to code here, although you have to know how it functions for next use**\n",
    "\n",
    "Given a feature $X_j$, a value $v \\in X_j$ and a class $c_k$, the likeelihood is calculated:\n",
    "$$ P(X_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ and } X_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
    "\n",
    "This function aims to generate parameters $\\theta$. \n",
    "In our case, paramters are diffrent from those of *logistic regrssion*.\n",
    "They are a dictionary (map) with two entries:\n",
    "- \"prior\" key having a dictionary as value, having \"v\" a list of values and \"p\" a list of their respective probabilities.\n",
    "- \"likelihood\" a list of dictinaries reprsnting statistics of each feature (the same order of $X$ features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prior': {'v': array(['no', 'yes'], dtype='<U3'),\n",
       "  'p': [-1.0296194171811581, -0.4418327522790392]},\n",
       " 'likelihood': [{'v': array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'),\n",
       "   'p': array([[-2.19722458, -0.95551145],\n",
       "          [-1.09861229, -1.178655  ],\n",
       "          [-0.81093022, -1.46633707],\n",
       "          [-2.19722458, -2.56494936]])},\n",
       "  {'v': array(['cool', 'hot', 'mild', '<UNK>'], dtype='<U8'),\n",
       "   'p': array([[-1.5040774 , -1.178655  ],\n",
       "          [-1.09861229, -1.46633707],\n",
       "          [-1.09861229, -0.95551145],\n",
       "          [-2.19722458, -2.56494936]])},\n",
       "  {'v': array(['high', 'normal', '<UNK>'], dtype='<U8'),\n",
       "   'p': array([[-0.47000363, -1.09861229],\n",
       "          [-1.38629436, -0.5389965 ],\n",
       "          [-2.07944154, -2.48490665]])},\n",
       "  {'v': array(['no', 'yes', '<UNK>'], dtype='<U8'),\n",
       "   'p': array([[-0.98082925, -0.5389965 ],\n",
       "          [-0.69314718, -1.09861229],\n",
       "          [-2.07944154, -2.48490665]])}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_NB(X: np.ndarray, \n",
    "        Y: np.ndarray,\n",
    "        alpha=0.\n",
    "        ) -> object: \n",
    "    \n",
    "    Theta   = {'prior': {}, 'likelihood': []}\n",
    "\n",
    "    Theta['prior']['v'], Theta['prior']['p'] = fit_prior(Y)\n",
    "\n",
    "    for j in range(X.shape[1]): \n",
    "        likelihood = {}\n",
    "        likelihood['v'], likelihood['p'] = fit_likelihood(X[:, j], Y, Theta['prior']['v'], alpha=alpha)\n",
    "        Theta['likelihood'].append(likelihood)\n",
    "    \n",
    "    return Theta\n",
    "\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# {'prior': {'v': array(['no', 'yes'], dtype='<U3'), \n",
    "#    'p': array([-1.02961942, -0.44183275])}, \n",
    "#    'likelihood': [{'v': array(['overcast', 'rainy', 'sunny', '<UNK>'], dtype='<U8'), \n",
    "#    'p': array([[-2.19722458, -0.95551145], \n",
    "#                     [-1.09861229, -1.178655 ], \n",
    "#                     [-0.81093022, -1.46633707], \n",
    "#                     [-2.19722458, -2.56494936]])}, \n",
    "#     {'v': array(['cool', 'hot', 'mild', '<UNK>'], dtype='<U8'), \n",
    "#       'p': array([[-1.5040774 , -1.178655 ], \n",
    "#                        [-1.09861229, -1.46633707], \n",
    "#                        [-1.09861229, -0.95551145], \n",
    "#                        [-2.19722458, -2.56494936]])}, \n",
    "#     {'v': array(['high', 'normal', '<UNK>'], dtype='<U8'), \n",
    "#       'p': array([[-0.47000363, -1.09861229], \n",
    "#                        [-1.38629436, -0.5389965 ], \n",
    "#                        [-2.07944154, -2.48490665]])}, \n",
    "#     {'v': array(['no', 'yes', '<UNK>'], dtype='<U8'), \n",
    "#       'p': array([[-0.98082925, -0.5389965 ], \n",
    "#                        [-0.69314718, -1.09861229], \n",
    "#                        [-2.07944154, -2.48490665]])}]}\n",
    "#---------------------------------------------------------------------\n",
    "Theta_play = fit_NB(Xplay, Yplay, alpha=1)\n",
    "\n",
    "Theta_play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.4. Multinomial prediction\n",
    "Let's examine prediction equation: \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "Our goal is to calculate approximate probabilities of all classes given a sample like indicated in the next equation:\n",
    "$$P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "- Given one sample $x$, we have to return a list of probabilities. \n",
    "- If prior=false, we must not consider prior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can use this function in the next implimentation\n",
    "# It takes a list of unique values V and a given value v\n",
    "# It returns the position of v in V\n",
    "# If v does not exist in V, it rturns -1\n",
    "def find_idx(V: np.ndarray, v: str) -> int:\n",
    "    k = np.argwhere(V == v).flatten()\n",
    "    if len(k):\n",
    "        return k[0]\n",
    "    return -1\n",
    "\n",
    "# TODO: Prediction\n",
    "def predict_NB_1(Xi    : np.ndarray, \n",
    "            Theta: object,  \n",
    "            add_prior: bool  = True\n",
    "           ) -> List[float]: \n",
    "\n",
    "    return None\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array([-6.81036293, -5.8230459 ]), array([-4.68213123, -3.99491878]))\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "X_t = np.array([\n",
    "    ['rainy', 'cool', 'normal', 'yes'],\n",
    "    ['snowy', 'cool', 'normal', 'yes'],\n",
    "    ['sunny', 'hot' , 'normal', 'no']\n",
    "])\n",
    "\n",
    "predict_NB_1(X_t[1, :], Theta_play), predict_NB_1(X_t[0, :], Theta_play, add_prior=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.5. Final product\n",
    "\n",
    "**>> Nothing to code here**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 36\u001b[0m\n\u001b[0;32m     29\u001b[0m cnb\u001b[38;5;241m.\u001b[39mfit(Xplay, Yplay, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     30\u001b[0m X_t \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\n\u001b[0;32m     31\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrainy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     32\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcool\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     33\u001b[0m     [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msunny\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhot\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     34\u001b[0m ])\n\u001b[1;32m---> 36\u001b[0m cnb\u001b[38;5;241m.\u001b[39mpredict(X_t), cnb\u001b[38;5;241m.\u001b[39mpredict(X_t, prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[18], line 16\u001b[0m, in \u001b[0;36mOurCategoricalNB.predict\u001b[1;34m(self, X, add_prior, prob)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prob:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Y_pred\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mchoose(np\u001b[38;5;241m.\u001b[39margmax(Y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTheta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprior\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1242\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1157\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124margmax\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mD:\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "class OurCategoricalNB(object): \n",
    "        \n",
    "    def fit(self, X, Y, alpha=0.):\n",
    "        self.Theta = fit_NB(X, Y, alpha=alpha)\n",
    "    \n",
    "    def predict(self, X, add_prior=True, prob=False): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            Y_pred.append(predict_NB_1(X[i,:], self.Theta, add_prior=add_prior))\n",
    "        \n",
    "        Y_pred = np.array(Y_pred)\n",
    "\n",
    "        if prob:\n",
    "            return Y_pred\n",
    "\n",
    "        return np.choose(np.argmax(Y_pred, axis=1), self.Theta['prior']['v'])\n",
    "\n",
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (array(['yes', 'yes', 'yes'], dtype='<U3'),\n",
    "#  array([[-5.9348942 , -3.11351531],\n",
    "#         [-6.22257627, -3.68887945],\n",
    "#         [-5.73009978, -3.32993436]]))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "cnb = OurCategoricalNB()\n",
    "cnb.fit(Xplay, Yplay, alpha=1)\n",
    "X_t = np.array([\n",
    "    ['rainy', 'cool', 'normal', 'yes'],\n",
    "    ['snowy', 'cool', 'normal', 'yes'],\n",
    "    ['sunny', 'hot' , 'normal', 'no']\n",
    "])\n",
    "\n",
    "cnb.predict(X_t), cnb.predict(X_t, prob=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application and Analysis\n",
    "\n",
    "In this section, we will test different concepts by running an experiment, formulating a hypothesis and trying to justify it.\n",
    "\n",
    "### II.1. Prior probability \n",
    "\n",
    "We want to test the effect of prior probability.\n",
    "To do this, we trained two models:\n",
    "1. With prior probability\n",
    "1. Without prior probability (It considers a uniform distribution of classes)\n",
    "\n",
    "To test whether the models have adapted well to the training dataset, we will test them on the same dataset and calculate the classification ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considring prior probability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "No prior probability\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       0.67      0.80      0.73         5\n",
      "         yes       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.77      0.79      0.78        14\n",
      "weighted avg       0.80      0.79      0.79        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_withPrior     = CategoricalNB(alpha=1.0, fit_prior=True )\n",
    "nb_noPrior       = CategoricalNB(alpha=1.0, fit_prior=False)\n",
    "\n",
    "enc         = OrdinalEncoder()\n",
    "Xplay_tf    = enc.fit_transform(Xplay)\n",
    "nb_withPrior.fit(Xplay_tf, Yplay)\n",
    "nb_noPrior.fit(Xplay_tf, Yplay)\n",
    "\n",
    "Ypred_withPrior = nb_withPrior.predict(Xplay_tf)\n",
    "Ypred_noPrior = nb_noPrior.predict(Xplay_tf)\n",
    "\n",
    "\n",
    "print( 'Considring prior probability'  )\n",
    "print(classification_report(Yplay, Ypred_withPrior))\n",
    "\n",
    "print( 'No prior probability'  )\n",
    "print(classification_report(Yplay, Ypred_noPrior))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice, indicating if prior probability is useful in this case?\n",
    "1. How does this probability affect the outcome?\n",
    "1. When are we sure that using this probability is unnecessary? \n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. From the results, it’s clear that the model considering prior probability has a higher overall accuracy (93%) compared to the model that doesn’t (79%). This indicates that in this case, prior probability is useful for improving the model’s performance\n",
    "\n",
    "\n",
    "1. Prior probability provides an indication of the overall likelihood of a class in the dataset. When considered, it can help guide the model’s predictions, especially in cases where the data is imbalanced. In this case, it appears to have helped the model generalize better, as evidenced by the higher accuracy.\n",
    "\n",
    "1. The use of prior probability might be less useful when the classes are perfectly balanced, i.e., there is an equal number of samples for each class. In this case, the prior probability for each class would be the same and wouldn’t provide additional information to the model. Also, if the features allow for a clear separation of classes, prior probability might have less impact on the model’s performance. However, it’s generally recommended to include it, as it can help improve the model’s robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Smoothing\n",
    "\n",
    "We want to test the Lidstone smoothing's effect.\n",
    "To do this, we trained three models:\n",
    "1. alpha = 1 (Laplace smoothing)\n",
    "1. alpha = 0.5\n",
    "1. alpha = 0 (without smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha = 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Alpha = 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          no       1.00      0.80      0.89         5\n",
      "         yes       0.90      1.00      0.95         9\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.95      0.90      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/env/ml/lib/python3.10/site-packages/sklearn/naive_bayes.py:1504: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(smoothed_cat_count) - np.log(smoothed_class_count.reshape(-1, 1))\n"
     ]
    }
   ],
   "source": [
    "NBC_10 = CategoricalNB(alpha = 1.0 )\n",
    "NBC_05 = CategoricalNB(alpha = 0.5 )\n",
    "NBC_00 = CategoricalNB(alpha = 0.0 )\n",
    "\n",
    "NBC_10.fit( Xplay_tf,   Yplay )\n",
    "NBC_05.fit( Xplay_tf,   Yplay )\n",
    "NBC_00.fit( Xplay_tf,   Yplay )\n",
    "\n",
    "Y_10   = NBC_10.predict(Xplay_tf)\n",
    "Y_05   = NBC_05.predict(Xplay_tf)\n",
    "Y_00   = NBC_00.predict(Xplay_tf)\n",
    "\n",
    "\n",
    "print(          'Alpha = 1.0'             )\n",
    "print(classification_report(Yplay, Y_10))\n",
    "\n",
    "print(          'Alpha = 0.5'             )\n",
    "print(classification_report(Yplay, Y_05))\n",
    "\n",
    "print(          'Alpha = 0.0'             )\n",
    "print(classification_report(Yplay, Y_00))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice, indicating if smoothing affects performance in this case?\n",
    "1. Based on the past answeer, Why? \n",
    "1. Why do we get a \"RuntimeWarning: divide by zero\" error? \n",
    "1. What is the benefit of smoothing (generally; not just for this case)?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. it appears that the smoothing parameter alpha does not significantly affect the performance of the model in this specific case. The accuracy, precision, recall, and f1-score remain the same for all three models (with alpha values of 1.0, 0.5, and 0.0). .\n",
    "\n",
    "\n",
    "1.  in our case , all categories in the test set (which is the same as our training set) are already represented in the training set. Therefore, there are no instances of zero-frequency problem. This is why changing the alpha value does not affect the performance of your model.\n",
    "\n",
    "\n",
    "1.  this warning can occur when the model encounters a category in the test data that was not present in the training data, and no smoothing has been applied. When the model tries to calculate the probability of this category, it involves a division by its count in the training data. Since the category was not in the training data, its count is zero, leading to a division by zero\n",
    "\n",
    "\n",
    "1. The general benefit of smoothing is to prevent zero probabilities in probabilistic models like Naive Bayes. By ensuring that no category has a zero probability, smoothing allows the model to handle unseen data more effectively. This can lead to more robust and accurate predictions when the model is applied to new, unseen data. It’s particularly useful in domains like natural language processing, where the vocabulary can be vast and the training data might not cover all possible words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.3. Naive Bayes performance\n",
    "\n",
    "Naive Bayes is known to generate powerful models when it comes to classifying textual documents.\n",
    "We want to test this proposition using spam detection over [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset) dataset.\n",
    "\n",
    "Each message is represented using term frequency (TF), where a word is considered as a feature.\n",
    "In this case, a message is represented by a vector of frequencies (how many times each word appeared in the message).\n",
    "We want to compare these models:\n",
    "1. Multinomial Naive Bayes (MNB)\n",
    "1. Gaussian Naive Bayes (GNB)\n",
    "1. Logistic Regression (LR) \n",
    "1. Decision Tree (DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text class\n",
       "0  Go until jurong point, crazy.. Available only ...   ham\n",
       "1                      Ok lar... Joking wif u oni...   ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  spam\n",
       "3  U dun say so early hor... U c already then say...   ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...   ham"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the dataset\n",
    "messages = pd.read_csv('data/spam.csv', encoding='latin-1')\n",
    "# renaming features: text and class\n",
    "messages = messages.rename(columns={'v1': 'class', 'v2': 'text'})\n",
    "# keeping only these two features\n",
    "messages = messages.filter(['text', 'class'])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Train time</th>\n",
       "      <th>Test time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial Naive Bayes (MNB)</td>\n",
       "      <td>0.559776</td>\n",
       "      <td>0.029081</td>\n",
       "      <td>0.987179</td>\n",
       "      <td>0.927711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaussian Naive Bayes  (GNB)</td>\n",
       "      <td>0.551694</td>\n",
       "      <td>0.127377</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.891566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression (LR)</td>\n",
       "      <td>0.555998</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.855422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree (DT)</td>\n",
       "      <td>13.080956</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.909677</td>\n",
       "      <td>0.849398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Algorithm  Train time  Test time  Precision    Recall\n",
       "0  Multinomial Naive Bayes (MNB)    0.559776   0.029081   0.987179  0.927711\n",
       "1    Gaussian Naive Bayes  (GNB)    0.551694   0.127377   0.616667  0.891566\n",
       "2       Logistic Regression (LR)    0.555998   0.028857   0.986111  0.855422\n",
       "3             Decision Tree (DT)   13.080956   0.016129   0.909677  0.849398"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    MultinomialNB(),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(solver='lbfgs'),\n",
    "    #solver=sag is slower; so I chose the fastest\n",
    "    DecisionTreeClassifier()\n",
    "]\n",
    "\n",
    "algos = [\n",
    "    'Multinomial Naive Bayes (MNB)', \n",
    "    'Gaussian Naive Bayes  (GNB)', \n",
    "    'Logistic Regression (LR)', \n",
    "    'Decision Tree (DT)'\n",
    "]\n",
    "\n",
    "perf = {\n",
    "    'train_time': [],\n",
    "    'test_time' : [],\n",
    "    'recall'    : [],\n",
    "    'precision' : []\n",
    "}\n",
    "\n",
    "\n",
    "msg_train, msg_test, Y_train, Y_test = train_test_split(messages['text'] ,\n",
    "                                                        messages['class'],\n",
    "                                                        test_size    = 0.2, \n",
    "                                                        random_state = 0  )\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train          = count_vectorizer.fit_transform(msg_train).toarray()\n",
    "X_test           = count_vectorizer.transform    (msg_test ).toarray()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    # ==================================\n",
    "    # TRAIN \n",
    "    # ==================================\n",
    "    start_time = timeit.default_timer()\n",
    "    model.fit(X_train, Y_train)\n",
    "    perf['train_time'].append(timeit.default_timer() - start_time)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    start_time = timeit.default_timer()\n",
    "    Y_pred     = model.predict(X_test)\n",
    "    perf['test_time'].append(timeit.default_timer() - start_time)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # In here, we are interrested in \"spam\" class which is our positive class\n",
    "    perf['precision'].append(precision_score(Y_test, Y_pred, pos_label='spam'))\n",
    "    perf['recall'   ].append(recall_score   (Y_test, Y_pred, pos_label='spam'))\n",
    "\n",
    "    \n",
    "pd.DataFrame({\n",
    "    'Algorithm' : algos,\n",
    "    'Train time': perf['train_time'],\n",
    "    'Test time' : perf['test_time'],\n",
    "    'Precision' : perf['precision'],\n",
    "    'Recall'    : perf['recall']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Analyze the results**\n",
    "\n",
    "1. What do you notice about training time? (order the algorithms)\n",
    "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to training time)\n",
    "1. What do you notice about the testing time? (order the algorithms)\n",
    "1. Why did we get these results based on the algorithms? (discuss each algorithm with respect to testing time)\n",
    "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the two algorithms?\n",
    "1. Why is the Gaussian model less efficient than the multinomial based on the nature of the problem/data?\n",
    "\n",
    "**Answer**\n",
    "\n",
    "1. based on the results : here’s how the algorithms rank from fastest to slowest:\n",
    "\n",
    "Logistic Regression (LR): With a training time of approximately 0.556 seconds, this is the fastest algorithm among the ones you tested.\n",
    "Multinomial Naive Bayes (MNB): This algorithm took approximately 0.560 seconds to train, making it the second fastest.\n",
    "Gaussian Naive Bayes (GNB): With a training time of approximately 0.552 seconds, this is the third fastest algorithm.\n",
    "Decision Tree (DT): This algorithm took significantly longer to train than the others, with a training time of approximately 13.081 seconds. It is the slowest among the tested algorithms\n",
    "\n",
    "\n",
    "\n",
    "1.Multinomial Naive Bayes (MNB): This algorithm is fast because it treats features independently, which simplifies the computation. It’s particularly efficient with discrete data, like word counts for text classification.\n",
    "Gaussian Naive Bayes (GNB): Similar to MNB, GNB assumes feature independence, making it computationally efficient. However, it’s designed for continuous data and may require additional time to estimate the mean and variance of each class.\n",
    "Logistic Regression (LR): LR uses a numerical optimization technique to find the best parameters, which can be computationally intensive, especially for high-dimensional data. However, it’s still relatively fast compared to more complex models.\n",
    "Decision Tree (DT): DTs can be slower to train because they require more complex computations to determine the best splits. The training time can increase significantly with the number of features and depth of the tree.\n",
    "\n",
    "\n",
    "1. algorithms rank from fastest to slowest:\n",
    "\n",
    "Decision Tree (DT): With a testing time of approximately 0.016 seconds, this is the fastest algorithm among the ones you tested.\n",
    "Logistic Regression (LR): This algorithm took approximately 0.029 seconds to test, making it the second fastest.\n",
    "Multinomial Naive Bayes (MNB): This algorithm took approximately 0.029 seconds to test, making it the third fastest.\n",
    "Gaussian Naive Bayes (GNB): With a testing time of approximately 0.127 seconds, this is the slowest among the tested algorithms.\n",
    "\n",
    "\n",
    "1. Decision Tree (DT): Decision Trees are fast at making predictions once they are trained. The testing phase involves traversing the tree from root to leaf, which is computationally efficient.\n",
    "Logistic Regression (LR) and Multinomial Naive Bayes (MNB): Both these algorithms are relatively fast at making predictions. They involve simple mathematical operations on the feature values and model parameters.\n",
    "Gaussian Naive Bayes (GNB): This algorithm might take longer in the testing phase compared to others because it needs to calculate the probability of a feature value given a class using the Gaussian distribution, which can be more computationally intensive\n",
    "\n",
    "\n",
    "1. The Multinomial Naive Bayes model is designed for discrete data. It works well with data that can be expressed as counts, such as the frequency of words in text. This makes it a good choice for text classification tasks, and it can be more efficient when dealing with this type of data.\n",
    "\n",
    "On the other hand, the Gaussian Naive Bayes model is designed for continuous data. It assumes that the data for each label is drawn from a Gaussian distribution. While this is a good assumption for many types of continuous data, it may not be as efficient for discrete data like word counts. The Gaussian model needs to calculate the mean and standard deviation for each class, which can be computationally intensive.\n",
    "\n",
    "So, while both models are useful, the Multinomial Naive Bayes model can be more efficient for tasks like text classification. The Gaussian Naive Bayes model might be a better choice for tasks involving continuous data\n",
    "\n",
    "\n",
    "1. The Multinomial Naive Bayes model is designed for discrete count data, like the frequency of words in a text. It’s particularly efficient when dealing with this type of data because it simply counts the occurrences of features and calculates probabilities.\n",
    "\n",
    "On the other hand, the Gaussian Naive Bayes model is designed for continuous data and assumes that the data for each class is drawn from a Gaussian distribution. This involves calculating the mean and standard deviation for each feature, which can be computationally intensive. Therefore, when applied to discrete count data (like word counts), the Gaussian model can be less efficient than the Multinomial model.\n",
    "\n",
    "So, the nature of your data and the problem you’re trying to solve can determine which model is more efficient. For text classification problems with count data, the Multinomial Naive Bayes model is often a better choice. For problems involving continuous data, the Gaussian Naive Bayes model might be more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _____    __                                              _               \n",
      " |_   _|  / _|                                            | |              \n",
      "   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \n",
      "   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \n",
      "  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \n",
      " |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \n",
      "                   __/ |                     __/ |                         \n",
      "                  |___/                     |___/                          \n",
      "  _     _       _            __                                            \n",
      " | |   | |     (_)          / _|                 _                         \n",
      " | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \n",
      " | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \n",
      " | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \n",
      "  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \n",
      "                                                |/                         \n",
      "                                                                           \n",
      "                                                                           \n",
      "                                                                           \n",
      "  _   _    ___    _   _      __ _   _ __    ___                            \n",
      " | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \n",
      " | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \n",
      "  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \n",
      "   __/ |                                                                   \n",
      "  |___/                                                                    \n",
      "                    _                                                __    \n",
      "                   | |                                            _  \\ \\   \n",
      "  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \n",
      " | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \n",
      " | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \n",
      " |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \n",
      "                                                                     /_/   \n",
      "                                                                           \n"
     ]
    }
   ],
   "source": [
    "print(\"  _____    __                                              _               \")\n",
    "print(\" |_   _|  / _|                                            | |              \")\n",
    "print(\"   | |   | |_     _   _    ___    _   _      __ _    ___  | |_             \")\n",
    "print(\"   | |   |  _|   | | | |  / _ \\  | | | |    / _` |  / _ \\ | __|            \")\n",
    "print(\"  _| |_  | |     | |_| | | (_) | | |_| |   | (_| | |  __/ | |_             \")\n",
    "print(\" |_____| |_|      \\__, |  \\___/   \\__,_|    \\__, |  \\___|  \\__|            \")\n",
    "print(\"                   __/ |                     __/ |                         \")\n",
    "print(\"                  |___/                     |___/                          \")\n",
    "print(\"  _     _       _            __                                            \")\n",
    "print(\" | |   | |     (_)          / _|                 _                         \")\n",
    "print(\" | |_  | |__    _   ___    | |_    __ _   _ __  (_)                        \")\n",
    "print(\" | __| | '_ \\  | | / __|   |  _|  / _` | | '__|                            \")\n",
    "print(\" | |_  | | | | | | \\__ \\   | |   | (_| | | |     _                         \")\n",
    "print(\"  \\__| |_| |_| |_| |___/   |_|    \\__,_| |_|    ( )                        \")\n",
    "print(\"                                                |/                         \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"                                                                           \")\n",
    "print(\"  _   _    ___    _   _      __ _   _ __    ___                            \")\n",
    "print(\" | | | |  / _ \\  | | | |    / _` | | '__|  / _ \\                           \")\n",
    "print(\" | |_| | | (_) | | |_| |   | (_| | | |    |  __/                           \")\n",
    "print(\"  \\__, |  \\___/   \\__,_|    \\__,_| |_|     \\___|                           \")\n",
    "print(\"   __/ |                                                                   \")\n",
    "print(\"  |___/                                                                    \")\n",
    "print(\"                    _                                                __    \")\n",
    "print(\"                   | |                                            _  \\ \\   \")\n",
    "print(\"  _ __     ___     | |__    _   _   _ __ ___     __ _   _ __     (_)  | |  \")\n",
    "print(\" | '_ \\   / _ \\    | '_ \\  | | | | | '_ ` _ \\   / _` | | '_ \\         | |  \")\n",
    "print(\" | | | | | (_) |   | | | | | |_| | | | | | | | | (_| | | | | |    _   | |  \")\n",
    "print(\" |_| |_|  \\___/    |_| |_|  \\__,_| |_| |_| |_|  \\__,_| |_| |_|   (_)  | |  \")\n",
    "print(\"                                                                     /_/   \")\n",
    "print(\"                                                                           \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
